{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from files \n",
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print \"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
    "\"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are not removed as word vector uses full sentence too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review,'lxml').get_text().encode('ascii', 'ignore').decode('ascii')\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    review=re.sub(r'[^\\x00-\\x7f]',r' ',review)\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence,remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \".\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"...\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"... ...\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"....\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"..\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \".. .\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-03-22 14:33:03,766 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2017-03-22 14:33:03,891 : INFO : collecting all words and their counts\n",
      "2017-03-22 14:33:03,895 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-03-22 14:33:04,019 : INFO : PROGRESS: at sentence #10000, processed 225818 words, keeping 17776 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-22 14:33:04,112 : INFO : PROGRESS: at sentence #20000, processed 451941 words, keeping 24951 word types\n",
      "2017-03-22 14:33:04,196 : INFO : PROGRESS: at sentence #30000, processed 671563 words, keeping 30034 word types\n",
      "2017-03-22 14:33:04,270 : INFO : PROGRESS: at sentence #40000, processed 898095 words, keeping 34353 word types\n",
      "2017-03-22 14:33:04,348 : INFO : PROGRESS: at sentence #50000, processed 1117439 words, keeping 37766 word types\n",
      "2017-03-22 14:33:04,424 : INFO : PROGRESS: at sentence #60000, processed 1338704 words, keeping 40723 word types\n",
      "2017-03-22 14:33:04,500 : INFO : PROGRESS: at sentence #70000, processed 1561919 words, keeping 43334 word types\n",
      "2017-03-22 14:33:04,576 : INFO : PROGRESS: at sentence #80000, processed 1781516 words, keeping 45719 word types\n",
      "2017-03-22 14:33:04,657 : INFO : PROGRESS: at sentence #90000, processed 2005543 words, keeping 48137 word types\n",
      "2017-03-22 14:33:04,733 : INFO : PROGRESS: at sentence #100000, processed 2227539 words, keeping 50212 word types\n",
      "2017-03-22 14:33:04,812 : INFO : PROGRESS: at sentence #110000, processed 2447455 words, keeping 52087 word types\n",
      "2017-03-22 14:33:04,888 : INFO : PROGRESS: at sentence #120000, processed 2669380 words, keeping 54125 word types\n",
      "2017-03-22 14:33:04,974 : INFO : PROGRESS: at sentence #130000, processed 2895050 words, keeping 55851 word types\n",
      "2017-03-22 14:33:05,056 : INFO : PROGRESS: at sentence #140000, processed 3108018 words, keeping 57355 word types\n",
      "2017-03-22 14:33:05,134 : INFO : PROGRESS: at sentence #150000, processed 3333315 words, keeping 59057 word types\n",
      "2017-03-22 14:33:05,217 : INFO : PROGRESS: at sentence #160000, processed 3556007 words, keeping 60624 word types\n",
      "2017-03-22 14:33:05,296 : INFO : PROGRESS: at sentence #170000, processed 3779479 words, keeping 62083 word types\n",
      "2017-03-22 14:33:05,388 : INFO : PROGRESS: at sentence #180000, processed 4000383 words, keeping 63501 word types\n",
      "2017-03-22 14:33:05,474 : INFO : PROGRESS: at sentence #190000, processed 4225397 words, keeping 64805 word types\n",
      "2017-03-22 14:33:05,555 : INFO : PROGRESS: at sentence #200000, processed 4449461 words, keeping 66089 word types\n",
      "2017-03-22 14:33:05,635 : INFO : PROGRESS: at sentence #210000, processed 4671281 words, keeping 67400 word types\n",
      "2017-03-22 14:33:05,713 : INFO : PROGRESS: at sentence #220000, processed 4895782 words, keeping 68698 word types\n",
      "2017-03-22 14:33:05,796 : INFO : PROGRESS: at sentence #230000, processed 5118938 words, keeping 69967 word types\n",
      "2017-03-22 14:33:05,877 : INFO : PROGRESS: at sentence #240000, processed 5346160 words, keeping 71172 word types\n",
      "2017-03-22 14:33:05,950 : INFO : PROGRESS: at sentence #250000, processed 5560404 words, keeping 72357 word types\n",
      "2017-03-22 14:33:06,032 : INFO : PROGRESS: at sentence #260000, processed 5780435 words, keeping 73490 word types\n",
      "2017-03-22 14:33:06,114 : INFO : PROGRESS: at sentence #270000, processed 6002184 words, keeping 74773 word types\n",
      "2017-03-22 14:33:06,204 : INFO : PROGRESS: at sentence #280000, processed 6228560 words, keeping 76424 word types\n",
      "2017-03-22 14:33:06,292 : INFO : PROGRESS: at sentence #290000, processed 6451041 words, keeping 77848 word types\n",
      "2017-03-22 14:33:06,400 : INFO : PROGRESS: at sentence #300000, processed 6675510 words, keeping 79175 word types\n",
      "2017-03-22 14:33:06,469 : INFO : PROGRESS: at sentence #310000, processed 6901424 words, keeping 80500 word types\n",
      "2017-03-22 14:33:06,540 : INFO : PROGRESS: at sentence #320000, processed 7126113 words, keeping 81821 word types\n",
      "2017-03-22 14:33:06,647 : INFO : PROGRESS: at sentence #330000, processed 7348165 words, keeping 83043 word types\n",
      "2017-03-22 14:33:06,726 : INFO : PROGRESS: at sentence #340000, processed 7578074 words, keeping 84293 word types\n",
      "2017-03-22 14:33:06,805 : INFO : PROGRESS: at sentence #350000, processed 7800621 words, keeping 85444 word types\n",
      "2017-03-22 14:33:06,887 : INFO : PROGRESS: at sentence #360000, processed 8021003 words, keeping 86602 word types\n",
      "2017-03-22 14:33:06,971 : INFO : PROGRESS: at sentence #370000, processed 8249977 words, keeping 87728 word types\n",
      "2017-03-22 14:33:07,053 : INFO : PROGRESS: at sentence #380000, processed 8473695 words, keeping 88883 word types\n",
      "2017-03-22 14:33:07,137 : INFO : PROGRESS: at sentence #390000, processed 8704012 words, keeping 89918 word types\n",
      "2017-03-22 14:33:07,217 : INFO : PROGRESS: at sentence #400000, processed 8926400 words, keeping 90930 word types\n",
      "2017-03-22 14:33:07,301 : INFO : PROGRESS: at sentence #410000, processed 9148253 words, keeping 91895 word types\n",
      "2017-03-22 14:33:07,385 : INFO : PROGRESS: at sentence #420000, processed 9369259 words, keeping 92924 word types\n",
      "2017-03-22 14:33:07,471 : INFO : PROGRESS: at sentence #430000, processed 9597185 words, keeping 93942 word types\n",
      "2017-03-22 14:33:07,555 : INFO : PROGRESS: at sentence #440000, processed 9823180 words, keeping 94924 word types\n",
      "2017-03-22 14:33:07,640 : INFO : PROGRESS: at sentence #450000, processed 10047662 words, keeping 96050 word types\n",
      "2017-03-22 14:33:07,723 : INFO : PROGRESS: at sentence #460000, processed 10280030 words, keeping 97093 word types\n",
      "2017-03-22 14:33:07,805 : INFO : PROGRESS: at sentence #470000, processed 10507809 words, keeping 97941 word types\n",
      "2017-03-22 14:33:07,887 : INFO : PROGRESS: at sentence #480000, processed 10728609 words, keeping 98877 word types\n",
      "2017-03-22 14:33:07,973 : INFO : PROGRESS: at sentence #490000, processed 10955176 words, keeping 99877 word types\n",
      "2017-03-22 14:33:08,052 : INFO : PROGRESS: at sentence #500000, processed 11177162 words, keeping 100785 word types\n",
      "2017-03-22 14:33:08,148 : INFO : PROGRESS: at sentence #510000, processed 11402365 words, keeping 101704 word types\n",
      "2017-03-22 14:33:08,230 : INFO : PROGRESS: at sentence #520000, processed 11625829 words, keeping 102607 word types\n",
      "2017-03-22 14:33:08,313 : INFO : PROGRESS: at sentence #530000, processed 11850308 words, keeping 103407 word types\n",
      "2017-03-22 14:33:08,395 : INFO : PROGRESS: at sentence #540000, processed 12075151 words, keeping 104276 word types\n",
      "2017-03-22 14:33:08,486 : INFO : PROGRESS: at sentence #550000, processed 12300778 words, keeping 105153 word types\n",
      "2017-03-22 14:33:08,569 : INFO : PROGRESS: at sentence #560000, processed 12521774 words, keeping 106006 word types\n",
      "2017-03-22 14:33:08,655 : INFO : PROGRESS: at sentence #570000, processed 12750746 words, keeping 106795 word types\n",
      "2017-03-22 14:33:08,742 : INFO : PROGRESS: at sentence #580000, processed 12972874 words, keeping 107681 word types\n",
      "2017-03-22 14:33:08,828 : INFO : PROGRESS: at sentence #590000, processed 13198661 words, keeping 108519 word types\n",
      "2017-03-22 14:33:08,908 : INFO : PROGRESS: at sentence #600000, processed 13420412 words, keeping 109226 word types\n",
      "2017-03-22 14:33:08,993 : INFO : PROGRESS: at sentence #610000, processed 13642196 words, keeping 110105 word types\n",
      "2017-03-22 14:33:09,078 : INFO : PROGRESS: at sentence #620000, processed 13867798 words, keeping 110849 word types\n",
      "2017-03-22 14:33:09,163 : INFO : PROGRESS: at sentence #630000, processed 14092252 words, keeping 111627 word types\n",
      "2017-03-22 14:33:09,250 : INFO : PROGRESS: at sentence #640000, processed 14313941 words, keeping 112430 word types\n",
      "2017-03-22 14:33:09,338 : INFO : PROGRESS: at sentence #650000, processed 14538880 words, keeping 113209 word types\n",
      "2017-03-22 14:33:09,433 : INFO : PROGRESS: at sentence #660000, processed 14762193 words, keeping 113968 word types\n",
      "2017-03-22 14:33:09,522 : INFO : PROGRESS: at sentence #670000, processed 14984995 words, keeping 114659 word types\n",
      "2017-03-22 14:33:09,605 : INFO : PROGRESS: at sentence #680000, processed 15210487 words, keeping 115370 word types\n",
      "2017-03-22 14:33:09,690 : INFO : PROGRESS: at sentence #690000, processed 15432770 words, keeping 116155 word types\n",
      "2017-03-22 14:33:09,778 : INFO : PROGRESS: at sentence #700000, processed 15660682 words, keeping 116957 word types\n",
      "2017-03-22 14:33:09,867 : INFO : PROGRESS: at sentence #710000, processed 15884041 words, keeping 117621 word types\n",
      "2017-03-22 14:33:09,956 : INFO : PROGRESS: at sentence #720000, processed 16109599 words, keeping 118242 word types\n",
      "2017-03-22 14:33:10,039 : INFO : PROGRESS: at sentence #730000, processed 16335594 words, keeping 118970 word types\n",
      "2017-03-22 14:33:10,124 : INFO : PROGRESS: at sentence #740000, processed 16557221 words, keeping 119685 word types\n",
      "2017-03-22 14:33:10,213 : INFO : PROGRESS: at sentence #750000, processed 16777061 words, keeping 120315 word types\n",
      "2017-03-22 14:33:10,298 : INFO : PROGRESS: at sentence #760000, processed 16994917 words, keeping 120944 word types\n",
      "2017-03-22 14:33:10,390 : INFO : PROGRESS: at sentence #770000, processed 17222668 words, keeping 121719 word types\n",
      "2017-03-22 14:33:10,480 : INFO : PROGRESS: at sentence #780000, processed 17452356 words, keeping 122420 word types\n",
      "2017-03-22 14:33:10,565 : INFO : PROGRESS: at sentence #790000, processed 17679689 words, keeping 123088 word types\n",
      "2017-03-22 14:33:10,621 : INFO : collected 123507 word types from a corpus of 17798063 raw words and 795317 sentences\n",
      "2017-03-22 14:33:10,624 : INFO : Loading a fresh vocabulary\n",
      "2017-03-22 14:33:10,785 : INFO : min_count=40 retains 16490 unique words (13% of original 123507, drops 107017)\n",
      "2017-03-22 14:33:10,786 : INFO : min_count=40 leaves 17238929 word corpus (96% of original 17798063, drops 559134)\n",
      "2017-03-22 14:33:11,049 : INFO : deleting the raw counts dictionary of 123507 items\n",
      "2017-03-22 14:33:11,058 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2017-03-22 14:33:11,061 : INFO : downsampling leaves estimated 12749650 word corpus (74.0% of prior 17238929)\n",
      "2017-03-22 14:33:11,062 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2017-03-22 14:33:11,448 : INFO : resetting layer weights\n",
      "2017-03-22 14:33:13,385 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-03-22 14:33:13,388 : INFO : expecting 795317 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-03-22 14:33:16,181 : INFO : PROGRESS: at 0.01% examples, 2655 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:33:17,197 : INFO : PROGRESS: at 0.63% examples, 108122 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:18,203 : INFO : PROGRESS: at 1.23% examples, 166879 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:19,234 : INFO : PROGRESS: at 1.88% examples, 208057 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:33:20,242 : INFO : PROGRESS: at 2.51% examples, 236233 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:33:21,256 : INFO : PROGRESS: at 3.15% examples, 257299 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-22 14:33:22,270 : INFO : PROGRESS: at 3.76% examples, 271617 words/s, in_qsize 7, out_qsize 1\n",
      "2017-03-22 14:33:23,276 : INFO : PROGRESS: at 4.41% examples, 284999 words/s, in_qsize 4, out_qsize 0\n",
      "2017-03-22 14:33:24,305 : INFO : PROGRESS: at 5.02% examples, 294246 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:25,336 : INFO : PROGRESS: at 5.66% examples, 303238 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:33:26,351 : INFO : PROGRESS: at 6.30% examples, 310396 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:33:27,384 : INFO : PROGRESS: at 6.93% examples, 315943 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:28,401 : INFO : PROGRESS: at 7.56% examples, 321488 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:33:29,417 : INFO : PROGRESS: at 8.17% examples, 325456 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:33:30,430 : INFO : PROGRESS: at 8.81% examples, 330254 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-22 14:33:31,440 : INFO : PROGRESS: at 9.41% examples, 332997 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:32,451 : INFO : PROGRESS: at 10.03% examples, 336490 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:33:33,459 : INFO : PROGRESS: at 10.65% examples, 339237 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:33:34,502 : INFO : PROGRESS: at 11.29% examples, 341785 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:35,520 : INFO : PROGRESS: at 11.91% examples, 344347 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:36,529 : INFO : PROGRESS: at 12.53% examples, 346453 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:33:37,562 : INFO : PROGRESS: at 13.16% examples, 348278 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-22 14:33:38,569 : INFO : PROGRESS: at 13.79% examples, 350263 words/s, in_qsize 3, out_qsize 0\n",
      "2017-03-22 14:33:39,592 : INFO : PROGRESS: at 14.40% examples, 351475 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:40,596 : INFO : PROGRESS: at 15.02% examples, 352984 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:41,618 : INFO : PROGRESS: at 15.65% examples, 354681 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:42,628 : INFO : PROGRESS: at 16.29% examples, 356102 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:43,648 : INFO : PROGRESS: at 16.90% examples, 357111 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:44,677 : INFO : PROGRESS: at 17.54% examples, 358514 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:33:45,674 : INFO : PROGRESS: at 18.14% examples, 359288 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:46,690 : INFO : PROGRESS: at 18.77% examples, 360362 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:33:47,707 : INFO : PROGRESS: at 19.39% examples, 361246 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:33:48,706 : INFO : PROGRESS: at 20.00% examples, 362091 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:49,710 : INFO : PROGRESS: at 20.58% examples, 362327 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:33:50,713 : INFO : PROGRESS: at 21.19% examples, 362933 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-22 14:33:51,736 : INFO : PROGRESS: at 21.82% examples, 363764 words/s, in_qsize 3, out_qsize 0\n",
      "2017-03-22 14:33:52,746 : INFO : PROGRESS: at 22.45% examples, 364430 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:33:53,750 : INFO : PROGRESS: at 23.07% examples, 365042 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:33:54,763 : INFO : PROGRESS: at 23.71% examples, 365891 words/s, in_qsize 3, out_qsize 0\n",
      "2017-03-22 14:33:55,782 : INFO : PROGRESS: at 24.33% examples, 366354 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:33:56,805 : INFO : PROGRESS: at 24.96% examples, 366877 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:33:57,821 : INFO : PROGRESS: at 25.58% examples, 367567 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-22 14:33:58,821 : INFO : PROGRESS: at 26.21% examples, 368126 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-22 14:33:59,829 : INFO : PROGRESS: at 26.83% examples, 368542 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-22 14:34:00,839 : INFO : PROGRESS: at 27.46% examples, 369186 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:01,844 : INFO : PROGRESS: at 28.07% examples, 369669 words/s, in_qsize 4, out_qsize 0\n",
      "2017-03-22 14:34:02,851 : INFO : PROGRESS: at 28.68% examples, 370005 words/s, in_qsize 4, out_qsize 0\n",
      "2017-03-22 14:34:03,859 : INFO : PROGRESS: at 29.30% examples, 370406 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:04,864 : INFO : PROGRESS: at 29.92% examples, 370971 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:05,867 : INFO : PROGRESS: at 30.53% examples, 371245 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:06,877 : INFO : PROGRESS: at 31.16% examples, 371720 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:07,898 : INFO : PROGRESS: at 31.77% examples, 372158 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:08,913 : INFO : PROGRESS: at 32.39% examples, 372514 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:09,928 : INFO : PROGRESS: at 33.02% examples, 372966 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:10,950 : INFO : PROGRESS: at 33.65% examples, 373244 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:11,983 : INFO : PROGRESS: at 34.28% examples, 373573 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:13,005 : INFO : PROGRESS: at 34.90% examples, 373904 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:34:13,994 : INFO : PROGRESS: at 35.51% examples, 374104 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:15,019 : INFO : PROGRESS: at 36.15% examples, 374557 words/s, in_qsize 4, out_qsize 0\n",
      "2017-03-22 14:34:16,026 : INFO : PROGRESS: at 36.77% examples, 374812 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:17,042 : INFO : PROGRESS: at 37.30% examples, 374047 words/s, in_qsize 2, out_qsize 1\n",
      "2017-03-22 14:34:18,055 : INFO : PROGRESS: at 37.91% examples, 374328 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:19,056 : INFO : PROGRESS: at 38.52% examples, 374612 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-22 14:34:20,082 : INFO : PROGRESS: at 39.16% examples, 374845 words/s, in_qsize 4, out_qsize 0\n",
      "2017-03-22 14:34:21,094 : INFO : PROGRESS: at 39.78% examples, 375116 words/s, in_qsize 3, out_qsize 0\n",
      "2017-03-22 14:34:22,108 : INFO : PROGRESS: at 40.39% examples, 375355 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:23,108 : INFO : PROGRESS: at 41.02% examples, 375706 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:24,112 : INFO : PROGRESS: at 41.66% examples, 376035 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-22 14:34:25,112 : INFO : PROGRESS: at 42.28% examples, 376275 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:34:26,124 : INFO : PROGRESS: at 42.90% examples, 376474 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:27,138 : INFO : PROGRESS: at 43.53% examples, 376576 words/s, in_qsize 4, out_qsize 0\n",
      "2017-03-22 14:34:28,153 : INFO : PROGRESS: at 44.15% examples, 376749 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:29,161 : INFO : PROGRESS: at 44.78% examples, 377040 words/s, in_qsize 4, out_qsize 0\n",
      "2017-03-22 14:34:30,178 : INFO : PROGRESS: at 45.41% examples, 377270 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-22 14:34:31,187 : INFO : PROGRESS: at 46.03% examples, 377471 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:32,200 : INFO : PROGRESS: at 46.68% examples, 377743 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:33,211 : INFO : PROGRESS: at 47.30% examples, 377971 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:34,217 : INFO : PROGRESS: at 47.92% examples, 378140 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:35,233 : INFO : PROGRESS: at 48.53% examples, 378259 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:34:36,243 : INFO : PROGRESS: at 49.12% examples, 378219 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:37,246 : INFO : PROGRESS: at 49.73% examples, 378381 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:38,263 : INFO : PROGRESS: at 50.35% examples, 378503 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:39,272 : INFO : PROGRESS: at 50.98% examples, 378660 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:40,303 : INFO : PROGRESS: at 51.57% examples, 378645 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:41,329 : INFO : PROGRESS: at 52.19% examples, 378776 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:42,342 : INFO : PROGRESS: at 52.81% examples, 378901 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:43,358 : INFO : PROGRESS: at 53.43% examples, 378976 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:44,371 : INFO : PROGRESS: at 54.02% examples, 378902 words/s, in_qsize 4, out_qsize 0\n",
      "2017-03-22 14:34:45,428 : INFO : PROGRESS: at 54.60% examples, 378614 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-22 14:34:46,450 : INFO : PROGRESS: at 55.23% examples, 378754 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:47,444 : INFO : PROGRESS: at 55.85% examples, 378900 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:48,464 : INFO : PROGRESS: at 56.49% examples, 379161 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:49,467 : INFO : PROGRESS: at 57.10% examples, 379304 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-22 14:34:50,476 : INFO : PROGRESS: at 57.72% examples, 379367 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:51,493 : INFO : PROGRESS: at 58.33% examples, 379487 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-22 14:34:52,499 : INFO : PROGRESS: at 58.97% examples, 379677 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:34:53,516 : INFO : PROGRESS: at 59.58% examples, 379710 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:54,523 : INFO : PROGRESS: at 60.20% examples, 379886 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:34:55,540 : INFO : PROGRESS: at 60.84% examples, 380125 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-22 14:34:56,549 : INFO : PROGRESS: at 61.46% examples, 380217 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:34:57,555 : INFO : PROGRESS: at 62.08% examples, 380322 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-22 14:34:58,565 : INFO : PROGRESS: at 62.71% examples, 380420 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-22 14:34:59,572 : INFO : PROGRESS: at 63.33% examples, 380482 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:00,591 : INFO : PROGRESS: at 63.96% examples, 380637 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:35:01,592 : INFO : PROGRESS: at 64.59% examples, 380756 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:35:02,601 : INFO : PROGRESS: at 65.22% examples, 380906 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:35:03,604 : INFO : PROGRESS: at 65.83% examples, 380969 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:04,618 : INFO : PROGRESS: at 66.48% examples, 381159 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:05,641 : INFO : PROGRESS: at 67.11% examples, 381256 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:06,674 : INFO : PROGRESS: at 67.74% examples, 381351 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:07,678 : INFO : PROGRESS: at 68.35% examples, 381408 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:08,696 : INFO : PROGRESS: at 68.98% examples, 381552 words/s, in_qsize 0, out_qsize 0\n",
      "2017-03-22 14:35:09,703 : INFO : PROGRESS: at 69.59% examples, 381590 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:10,736 : INFO : PROGRESS: at 70.23% examples, 381736 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:11,757 : INFO : PROGRESS: at 70.85% examples, 381828 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:12,775 : INFO : PROGRESS: at 71.47% examples, 381896 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:13,786 : INFO : PROGRESS: at 72.09% examples, 382050 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:14,786 : INFO : PROGRESS: at 72.71% examples, 382109 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:15,801 : INFO : PROGRESS: at 73.33% examples, 382188 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:35:16,808 : INFO : PROGRESS: at 73.94% examples, 382255 words/s, in_qsize 3, out_qsize 0\n",
      "2017-03-22 14:35:17,819 : INFO : PROGRESS: at 74.57% examples, 382375 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:35:18,829 : INFO : PROGRESS: at 75.19% examples, 382458 words/s, in_qsize 4, out_qsize 0\n",
      "2017-03-22 14:35:19,822 : INFO : PROGRESS: at 75.81% examples, 382593 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-22 14:35:20,832 : INFO : PROGRESS: at 76.43% examples, 382621 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:35:21,862 : INFO : PROGRESS: at 77.06% examples, 382697 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:22,869 : INFO : PROGRESS: at 77.68% examples, 382814 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:35:23,884 : INFO : PROGRESS: at 78.29% examples, 382812 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:24,888 : INFO : PROGRESS: at 78.92% examples, 382915 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:35:25,900 : INFO : PROGRESS: at 79.53% examples, 382965 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:26,910 : INFO : PROGRESS: at 80.16% examples, 383074 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-22 14:35:27,924 : INFO : PROGRESS: at 80.78% examples, 383083 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:28,940 : INFO : PROGRESS: at 81.42% examples, 383238 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-22 14:35:29,941 : INFO : PROGRESS: at 82.03% examples, 383260 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:35:30,946 : INFO : PROGRESS: at 82.20% examples, 381214 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:31,992 : INFO : PROGRESS: at 82.38% examples, 379226 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:32,976 : INFO : PROGRESS: at 82.73% examples, 378081 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:33,983 : INFO : PROGRESS: at 83.37% examples, 378219 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:35,013 : INFO : PROGRESS: at 84.01% examples, 378375 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:36,046 : INFO : PROGRESS: at 84.65% examples, 378466 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:37,069 : INFO : PROGRESS: at 85.29% examples, 378629 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:35:38,081 : INFO : PROGRESS: at 85.92% examples, 378749 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:39,089 : INFO : PROGRESS: at 86.56% examples, 378846 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:35:40,104 : INFO : PROGRESS: at 87.18% examples, 378933 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:41,105 : INFO : PROGRESS: at 87.79% examples, 379007 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:42,115 : INFO : PROGRESS: at 88.42% examples, 379124 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:43,148 : INFO : PROGRESS: at 89.06% examples, 379234 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:44,176 : INFO : PROGRESS: at 89.69% examples, 379377 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:45,167 : INFO : PROGRESS: at 90.31% examples, 379465 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:46,171 : INFO : PROGRESS: at 90.93% examples, 379592 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:47,210 : INFO : PROGRESS: at 91.55% examples, 379655 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:48,250 : INFO : PROGRESS: at 91.78% examples, 378098 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:49,302 : INFO : PROGRESS: at 91.96% examples, 376231 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:50,311 : INFO : PROGRESS: at 92.28% examples, 375131 words/s, in_qsize 0, out_qsize 0\n",
      "2017-03-22 14:35:51,315 : INFO : PROGRESS: at 92.87% examples, 375112 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-22 14:35:52,345 : INFO : PROGRESS: at 93.49% examples, 375213 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:53,345 : INFO : PROGRESS: at 94.11% examples, 375327 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:35:54,344 : INFO : PROGRESS: at 94.72% examples, 375422 words/s, in_qsize 4, out_qsize 0\n",
      "2017-03-22 14:35:55,362 : INFO : PROGRESS: at 95.35% examples, 375531 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-22 14:35:56,382 : INFO : PROGRESS: at 95.97% examples, 375637 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:57,404 : INFO : PROGRESS: at 96.60% examples, 375699 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:58,424 : INFO : PROGRESS: at 97.23% examples, 375821 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:35:59,428 : INFO : PROGRESS: at 97.85% examples, 375963 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-22 14:36:00,448 : INFO : PROGRESS: at 98.48% examples, 376041 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-22 14:36:01,483 : INFO : PROGRESS: at 99.12% examples, 376117 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-22 14:36:02,499 : INFO : PROGRESS: at 99.74% examples, 376285 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-22 14:36:02,868 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-03-22 14:36:02,882 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-03-22 14:36:02,894 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-03-22 14:36:02,905 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-03-22 14:36:02,908 : INFO : training on 88990315 raw words (63753280 effective words) took 169.4s, 376320 effective words/s\n",
      "2017-03-22 14:36:02,914 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-03-22 14:36:03,155 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-03-22 14:36:03,157 : INFO : not storing attribute syn0norm\n",
      "2017-03-22 14:36:03,161 : INFO : not storing attribute cum_table\n",
      "2017-03-22 14:36:05,651 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vector based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-22 15:19:30,806 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2017-03-22 15:19:30,944 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2017-03-22 15:19:30,947 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-03-22 15:19:30,950 : INFO : setting ignored attribute cum_table to None\n",
      "2017-03-22 15:19:30,951 : INFO : loaded 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.vocab.keys())\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model.wv[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000. == 0. :\n",
    "           print \"Review %d of %d\" % (counter, len(reviews))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print \"Creating average feature vecs for test reviews\"\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a SVM to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#forest = RandomForestClassifier( n_estimators = 100 )\n",
    "from sklearn.svm import LinearSVC\n",
    "print \"Fitting a SVM to labeled training data...\"\n",
    "#forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "SVM=LinearSVC(dual=False)\n",
    "SVM.fit(trainDataVecs, train[\"sentiment\"])\n",
    "# Test & extract results \n",
    "result = SVM.predict( testDataVecs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors_SVM.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

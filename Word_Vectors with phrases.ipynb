{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from files \n",
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print \"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
    "\"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review,'lxml').get_text().encode('ascii', 'ignore').decode('ascii')\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "bigram = Phrases()\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( bigram,review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    review=re.sub(r'[^\\x00-\\x7f]',r' ',review)\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentence=review_to_wordlist( raw_sentence,remove_stopwords )\n",
    "            sentences.append(sentence)\n",
    "            bigram.add_vocab([sentence])\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \".\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"...\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"... ...\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"....\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"..\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \".. .\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(bigram,review, tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(bigram,review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795317"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word vector Training using phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-22 18:05:53,943 : INFO : collecting all words and their counts\n",
      "2017-03-22 18:05:53,953 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-22 18:05:55,714 : INFO : PROGRESS: at sentence #10000, processed 212804 words, keeping 21630 word types\n",
      "2017-03-22 18:05:57,471 : INFO : PROGRESS: at sentence #20000, processed 426299 words, keeping 31596 word types\n",
      "2017-03-22 18:05:59,095 : INFO : PROGRESS: at sentence #30000, processed 633297 words, keeping 38610 word types\n",
      "2017-03-22 18:06:00,776 : INFO : PROGRESS: at sentence #40000, processed 847213 words, keeping 44647 word types\n",
      "2017-03-22 18:06:02,405 : INFO : PROGRESS: at sentence #50000, processed 1053991 words, keeping 49327 word types\n",
      "2017-03-22 18:06:04,053 : INFO : PROGRESS: at sentence #60000, processed 1263018 words, keeping 53427 word types\n",
      "2017-03-22 18:06:05,707 : INFO : PROGRESS: at sentence #70000, processed 1473470 words, keeping 56968 word types\n",
      "2017-03-22 18:06:07,328 : INFO : PROGRESS: at sentence #80000, processed 1680436 words, keeping 60179 word types\n",
      "2017-03-22 18:06:09,071 : INFO : PROGRESS: at sentence #90000, processed 1891232 words, keeping 63348 word types\n",
      "2017-03-22 18:06:10,717 : INFO : PROGRESS: at sentence #100000, processed 2100657 words, keeping 66071 word types\n",
      "2017-03-22 18:06:12,351 : INFO : PROGRESS: at sentence #110000, processed 2307927 words, keeping 68460 word types\n",
      "2017-03-22 18:06:13,987 : INFO : PROGRESS: at sentence #120000, processed 2517526 words, keeping 71019 word types\n",
      "2017-03-22 18:06:15,653 : INFO : PROGRESS: at sentence #130000, processed 2730433 words, keeping 73157 word types\n",
      "2017-03-22 18:06:17,329 : INFO : PROGRESS: at sentence #140000, processed 2931470 words, keeping 74995 word types\n",
      "2017-03-22 18:06:19,003 : INFO : PROGRESS: at sentence #150000, processed 3144191 words, keeping 77052 word types\n",
      "2017-03-22 18:06:20,818 : INFO : PROGRESS: at sentence #160000, processed 3354324 words, keeping 78937 word types\n",
      "2017-03-22 18:06:22,826 : INFO : PROGRESS: at sentence #170000, processed 3564957 words, keeping 80663 word types\n",
      "2017-03-22 18:06:24,592 : INFO : PROGRESS: at sentence #180000, processed 3773279 words, keeping 82328 word types\n",
      "2017-03-22 18:06:26,407 : INFO : PROGRESS: at sentence #190000, processed 3985238 words, keeping 83857 word types\n",
      "2017-03-22 18:06:28,519 : INFO : PROGRESS: at sentence #200000, processed 4196445 words, keeping 85376 word types\n",
      "2017-03-22 18:06:30,368 : INFO : PROGRESS: at sentence #210000, processed 4405660 words, keeping 86857 word types\n",
      "2017-03-22 18:06:32,161 : INFO : PROGRESS: at sentence #220000, processed 4617406 words, keeping 88331 word types\n",
      "2017-03-22 18:06:34,032 : INFO : PROGRESS: at sentence #230000, processed 4828200 words, keeping 89743 word types\n",
      "2017-03-22 18:06:36,289 : INFO : PROGRESS: at sentence #240000, processed 5042409 words, keeping 91095 word types\n",
      "2017-03-22 18:06:38,319 : INFO : PROGRESS: at sentence #250000, processed 5244295 words, keeping 92425 word types\n",
      "2017-03-22 18:06:40,217 : INFO : PROGRESS: at sentence #260000, processed 5451835 words, keeping 93672 word types\n",
      "2017-03-22 18:06:41,914 : INFO : PROGRESS: at sentence #270000, processed 5660840 words, keeping 95197 word types\n",
      "2017-03-22 18:06:43,588 : INFO : PROGRESS: at sentence #280000, processed 5874266 words, keeping 97180 word types\n",
      "2017-03-22 18:06:45,213 : INFO : PROGRESS: at sentence #290000, processed 6083849 words, keeping 98907 word types\n",
      "2017-03-22 18:06:46,881 : INFO : PROGRESS: at sentence #300000, processed 6295419 words, keeping 100468 word types\n",
      "2017-03-22 18:06:48,619 : INFO : PROGRESS: at sentence #310000, processed 6508367 words, keeping 102023 word types\n",
      "2017-03-22 18:06:50,437 : INFO : PROGRESS: at sentence #320000, processed 6720282 words, keeping 103551 word types\n",
      "2017-03-22 18:06:52,295 : INFO : PROGRESS: at sentence #330000, processed 6929658 words, keeping 104939 word types\n",
      "2017-03-22 18:06:54,056 : INFO : PROGRESS: at sentence #340000, processed 7146627 words, keeping 106368 word types\n",
      "2017-03-22 18:06:55,743 : INFO : PROGRESS: at sentence #350000, processed 7356469 words, keeping 107651 word types\n",
      "2017-03-22 18:06:57,427 : INFO : PROGRESS: at sentence #360000, processed 7564339 words, keeping 108928 word types\n",
      "2017-03-22 18:06:59,153 : INFO : PROGRESS: at sentence #370000, processed 7780401 words, keeping 110183 word types\n",
      "2017-03-22 18:07:01,028 : INFO : PROGRESS: at sentence #380000, processed 7991143 words, keeping 111455 word types\n",
      "2017-03-22 18:07:02,737 : INFO : PROGRESS: at sentence #390000, processed 8208228 words, keeping 112589 word types\n",
      "2017-03-22 18:07:04,411 : INFO : PROGRESS: at sentence #400000, processed 8417771 words, keeping 113688 word types\n",
      "2017-03-22 18:07:06,079 : INFO : PROGRESS: at sentence #410000, processed 8626762 words, keeping 114724 word types\n",
      "2017-03-22 18:07:07,737 : INFO : PROGRESS: at sentence #420000, processed 8834925 words, keeping 115839 word types\n",
      "2017-03-22 18:07:09,648 : INFO : PROGRESS: at sentence #430000, processed 9049908 words, keeping 116930 word types\n",
      "2017-03-22 18:07:11,575 : INFO : PROGRESS: at sentence #440000, processed 9262898 words, keeping 117981 word types\n",
      "2017-03-22 18:07:13,263 : INFO : PROGRESS: at sentence #450000, processed 9474978 words, keeping 119179 word types\n",
      "2017-03-22 18:07:15,029 : INFO : PROGRESS: at sentence #460000, processed 9694179 words, keeping 120265 word types\n",
      "2017-03-22 18:07:16,733 : INFO : PROGRESS: at sentence #470000, processed 9908863 words, keeping 121168 word types\n",
      "2017-03-22 18:07:18,388 : INFO : PROGRESS: at sentence #480000, processed 10117298 words, keeping 122159 word types\n",
      "2017-03-22 18:07:20,075 : INFO : PROGRESS: at sentence #490000, processed 10330657 words, keeping 123222 word types\n",
      "2017-03-22 18:07:21,750 : INFO : PROGRESS: at sentence #500000, processed 10540018 words, keeping 124168 word types\n",
      "2017-03-22 18:07:23,454 : INFO : PROGRESS: at sentence #510000, processed 10752387 words, keeping 125128 word types\n",
      "2017-03-22 18:07:25,227 : INFO : PROGRESS: at sentence #520000, processed 10963022 words, keeping 126078 word types\n",
      "2017-03-22 18:07:26,933 : INFO : PROGRESS: at sentence #530000, processed 11174093 words, keeping 126916 word types\n",
      "2017-03-22 18:07:28,642 : INFO : PROGRESS: at sentence #540000, processed 11386131 words, keeping 127809 word types\n",
      "2017-03-22 18:07:30,328 : INFO : PROGRESS: at sentence #550000, processed 11599117 words, keeping 128724 word types\n",
      "2017-03-22 18:07:32,437 : INFO : PROGRESS: at sentence #560000, processed 11807504 words, keeping 129614 word types\n",
      "2017-03-22 18:07:34,211 : INFO : PROGRESS: at sentence #570000, processed 12023260 words, keeping 130432 word types\n",
      "2017-03-22 18:07:35,905 : INFO : PROGRESS: at sentence #580000, processed 12232919 words, keeping 131339 word types\n",
      "2017-03-22 18:07:37,601 : INFO : PROGRESS: at sentence #590000, processed 12445873 words, keeping 132209 word types\n",
      "2017-03-22 18:07:39,243 : INFO : PROGRESS: at sentence #600000, processed 12654919 words, keeping 132942 word types\n",
      "2017-03-22 18:07:40,913 : INFO : PROGRESS: at sentence #610000, processed 12864057 words, keeping 133847 word types\n",
      "2017-03-22 18:07:42,592 : INFO : PROGRESS: at sentence #620000, processed 13076695 words, keeping 134613 word types\n",
      "2017-03-22 18:07:44,265 : INFO : PROGRESS: at sentence #630000, processed 13288378 words, keeping 135408 word types\n",
      "2017-03-22 18:07:45,911 : INFO : PROGRESS: at sentence #640000, processed 13497477 words, keeping 136226 word types\n",
      "2017-03-22 18:07:47,588 : INFO : PROGRESS: at sentence #650000, processed 13709716 words, keeping 137020 word types\n",
      "2017-03-22 18:07:49,252 : INFO : PROGRESS: at sentence #660000, processed 13920594 words, keeping 137797 word types\n",
      "2017-03-22 18:07:50,904 : INFO : PROGRESS: at sentence #670000, processed 14130636 words, keeping 138504 word types\n",
      "2017-03-22 18:07:52,582 : INFO : PROGRESS: at sentence #680000, processed 14343217 words, keeping 139226 word types\n",
      "2017-03-22 18:07:54,226 : INFO : PROGRESS: at sentence #690000, processed 14552836 words, keeping 140022 word types\n",
      "2017-03-22 18:07:55,915 : INFO : PROGRESS: at sentence #700000, processed 14767972 words, keeping 140835 word types\n",
      "2017-03-22 18:07:57,562 : INFO : PROGRESS: at sentence #710000, processed 14978573 words, keeping 141509 word types\n",
      "2017-03-22 18:07:59,226 : INFO : PROGRESS: at sentence #720000, processed 15191117 words, keeping 142141 word types\n",
      "2017-03-22 18:08:00,904 : INFO : PROGRESS: at sentence #730000, processed 15403909 words, keeping 142880 word types\n",
      "2017-03-22 18:08:02,553 : INFO : PROGRESS: at sentence #740000, processed 15612744 words, keeping 143605 word types\n",
      "2017-03-22 18:08:04,197 : INFO : PROGRESS: at sentence #750000, processed 15820051 words, keeping 144252 word types\n",
      "2017-03-22 18:08:05,815 : INFO : PROGRESS: at sentence #760000, processed 16025628 words, keeping 144890 word types\n",
      "2017-03-22 18:08:07,502 : INFO : PROGRESS: at sentence #770000, processed 16240477 words, keeping 145680 word types\n",
      "2017-03-22 18:08:09,211 : INFO : PROGRESS: at sentence #780000, processed 16456879 words, keeping 146390 word types\n",
      "2017-03-22 18:08:10,900 : INFO : PROGRESS: at sentence #790000, processed 16670962 words, keeping 147073 word types\n",
      "2017-03-22 18:08:11,788 : INFO : collected 147497 word types from a corpus of 16782536 raw words and 795317 sentences\n",
      "2017-03-22 18:08:11,789 : INFO : Loading a fresh vocabulary\n",
      "2017-03-22 18:08:11,950 : INFO : min_count=40 retains 18401 unique words (12% of original 147497, drops 129096)\n",
      "2017-03-22 18:08:11,950 : INFO : min_count=40 leaves 15952271 word corpus (95% of original 16782536, drops 830265)\n",
      "2017-03-22 18:08:12,434 : INFO : deleting the raw counts dictionary of 147497 items\n",
      "2017-03-22 18:08:12,453 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2017-03-22 18:08:12,457 : INFO : downsampling leaves estimated 11491541 word corpus (72.0% of prior 15952271)\n",
      "2017-03-22 18:08:12,460 : INFO : estimated required memory for 18401 words and 300 dimensions: 53362900 bytes\n",
      "2017-03-22 18:08:12,566 : INFO : resetting layer weights\n",
      "2017-03-22 18:08:13,153 : INFO : training model with 4 workers on 18401 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-03-22 18:08:13,154 : INFO : expecting 795317 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-03-22 18:08:13,256 : WARNING : train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2017-03-22 18:08:13,292 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-03-22 18:08:13,293 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-03-22 18:08:13,296 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-03-22 18:08:13,299 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-03-22 18:08:13,301 : INFO : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2017-03-22 18:08:13,302 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-03-22 18:08:13,305 : WARNING : supplied example count (0) did not equal expected count (3976585)\n",
      "2017-03-22 18:08:13,308 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-03-22 18:08:13,605 : INFO : saving Word2Vec object under 300features_40minwords_10context_phrases, separately None\n",
      "2017-03-22 18:08:13,607 : INFO : not storing attribute syn0norm\n",
      "2017-03-22 18:08:13,608 : INFO : not storing attribute cum_table\n",
      "2017-03-22 18:08:14,397 : INFO : saved 300features_40minwords_10context_phrases\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(bigram[sentences], workers=num_workers,size=num_features, min_count = min_word_count,window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context_phrases\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word vectors with phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-22 18:09:23,701 : INFO : loading Word2Vec object from 300features_40minwords_10context_phrases\n",
      "2017-03-22 18:09:23,880 : INFO : loading wv recursively from 300features_40minwords_10context_phrases.wv.* with mmap=None\n",
      "2017-03-22 18:09:23,881 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-03-22 18:09:23,884 : INFO : setting ignored attribute cum_table to None\n",
      "2017-03-22 18:09:23,888 : INFO : loaded 300features_40minwords_10context_phrases\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context_phrases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.vocab.keys())\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model.wv[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000. == 0. :\n",
    "           print \"Review %d of %d\" % (counter, len(reviews))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print \"Creating average feature vecs for test reviews\"\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a SVM to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "print \"Fitting a SVM to labeled training data...\"\n",
    "#forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "SVM=LinearSVC(dual=False)\n",
    "SVM.fit(trainDataVecs, train[\"sentiment\"])\n",
    "# Test & extract results \n",
    "result = SVM.predict( testDataVecs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_phrases_AverageVectors_SVM.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
